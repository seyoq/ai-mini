'use client'

import { useState, useRef, useEffect } from 'react';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';
import { Separator } from '@/components/ui/separator';
import { Phone, Mic, MicOff, Video, VideoOff } from 'lucide-react';
import { toast } from '@/hooks/use-toast';

type CallStatus = 'idle' | 'connecting' | 'connected' | 'calling';

interface SpeechRecognitionEvent extends Event {
  readonly resultIndex: number;
  readonly results: SpeechRecognitionResultList;
}

interface SpeechRecognitionResultList {
  [index: number]: SpeechRecognitionResult;
  length: number;
  item(index: number): SpeechRecognitionResult;
}

interface SpeechRecognitionResult {
  readonly isFinal: boolean;
  readonly length: number;
  item(index: number): SpeechRecognitionAlternative;
  [index: number]: SpeechRecognitionAlternative;
}

interface SpeechRecognitionAlternative {
  readonly transcript: string;
  readonly confidence: number;
}

export default function CallInterface() {
  const [callStatus, setCallStatus] = useState<CallStatus>('idle');
  const [userId, setUserId] = useState('');
  const [targetId, setTargetId] = useState('');
  const [isMuted, setIsMuted] = useState(false);
  const [isVideoOn, setIsVideoOn] = useState(false);
  const [isConnected, setIsConnected] = useState(false);
  const [transcript, setTranscript] = useState(''); // ìŒì„± í…ìŠ¤íŠ¸ ì €ì¥
const [receivedTranscript, setReceivedTranscript] = useState('');

  const localVideoRef = useRef<HTMLVideoElement>(null);
  const remoteVideoRef = useRef<HTMLVideoElement>(null);
  const localStreamRef = useRef<MediaStream | null>(null);
  const peerConnectionRef = useRef<RTCPeerConnection | null>(null);
  const ws = useRef<WebSocket | null>(null);


const recognitionRef = useRef<any>(null);


  const remoteAudioRef = useRef<HTMLAudioElement>(null);

useEffect(() => {
  if (!isConnected) return;

  const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
  if (!SpeechRecognition) {
    console.warn('ì´ ë¸Œë¼ìš°ì €ëŠ” SpeechRecognition APIë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
    return;
  }

  if (!recognitionRef.current) {
    recognitionRef.current = new SpeechRecognition();
  }

  const recognition = recognitionRef.current;
try {
  recognition.start();
} catch (e) {
  console.error('[SpeechRecognition] start ì‹¤íŒ¨:', e);
}
  recognition.lang = 'ko-KR';
  recognition.interimResults = true;
  recognition.continuous = true;

  recognition.onstart = () => console.log('[SpeechRecognition] ì‹œì‘ë¨');

  recognition.onend = () => {
    console.log('[SpeechRecognition] ì¢…ë£Œë¨, 1ì´ˆ í›„ ë‹¤ì‹œ ì‹œì‘ ì‹œë„');
    setTimeout(() => {
      try {
        recognition.start();
      } catch (error) {
        console.error('[SpeechRecognition] ì¬ì‹œì‘ ì—ëŸ¬:', error);
      }
    }, 1000);
  };

  recognition.onerror = (e: any) => {
    console.error('[SpeechRecognition] ì—ëŸ¬:', e.error);
    if (e.error === 'not-allowed' || e.error === 'service-not-allowed') {
      console.warn('ë§ˆì´í¬ ê¶Œí•œ ê±°ë¶€ë¨, ì¸ì‹ ì¤‘ì§€');
      recognition.stop();
    }
  };

  recognition.onresult = (event: SpeechRecognitionEvent) => {
  let finalTranscript = '';

  for (let i = event.resultIndex; i < event.results.length; i++) {
    const transcriptChunk = event.results[i][0].transcript;
    if (event.results[i].isFinal) {
      finalTranscript += transcriptChunk + ' ';
    }
  }

  if (finalTranscript.trim()) {
    console.log('ğŸ™ï¸ ì¸ì‹ëœ ìŒì„±:', finalTranscript);

    // ğŸ“¤ WebSocket ì „ì†¡ ë¡œê·¸
    if (ws.current && targetId) {
      console.log('ğŸ“¤ ì „ì†¡í•  transcript ë©”ì‹œì§€:', {
        type: 'transcript',
        to: targetId,
        text: finalTranscript,
      });

      ws.current.send(JSON.stringify({
        type: 'transcript',
        to: targetId,
        text: finalTranscript,
      }));
    }
  }
};





  return () => {
    recognition.stop();
  };
}, [isConnected]);


  // WebSocket ì—°ê²°
  useEffect(() => {
    if (!userId || ws.current) return;

    ws.current = new WebSocket(`wss://3cfdedc35589.ngrok-free.app/ws/${userId}`);

    ws.current.onopen = () => {
      console.log("âœ… WebSocket connected");
      setIsConnected(true);
      toast({ title: "WebSocket Connected" });
    };

    ws.current.onmessage = async (e) => {
      const message = JSON.parse(e.data);
      console.log("ğŸ“© Received:", message);

      switch (message.type) {
        case 'offer':
          await handleOffer(message);
          break;
        case 'ice':
          if (peerConnectionRef.current && message.candidate) {
            await peerConnectionRef.current.addIceCandidate(new RTCIceCandidate(message.candidate));
          }
          break;
        case 'answer':
          if (peerConnectionRef.current) {
            await peerConnectionRef.current.setRemoteDescription(new RTCSessionDescription(message.sdp));
            setCallStatus('connected');
          }
          break;
           case 'transcript':
      console.log('ğŸ“¥ transcript ë©”ì‹œì§€ ìˆ˜ì‹ ë¨:', message.text);
      if (message.text) {
        setReceivedTranscript(prev => {
          const updated = prev + message.text + ' ';
          console.log('ğŸ–Šï¸ í™”ë©´ì— í‘œì‹œí•  receivedTranscript:', updated);
          return updated;
        });
      }
      break;
      }
    };

    ws.current.onclose = () => {
      console.log("âŒ WebSocket closed");
      setIsConnected(false);
      setCallStatus('idle');
    };

    return () => {
      ws.current?.close();
      ws.current = null;
    };
  }, [userId]);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
  useEffect(() => {
    return () => {
      localStreamRef.current?.getTracks().forEach(t => t.stop());
      peerConnectionRef.current?.close();
    };
  }, []);

  // PeerConnection ìƒì„±
  const createPeerConnection = () => {
    const pc = new RTCPeerConnection();

    pc.onicecandidate = (event) => {
      if (event.candidate && ws.current && targetId) {
        ws.current.send(JSON.stringify({
          type: 'ice',
          candidate: event.candidate,
          to: targetId,
        }));
      }
    };

//    pc.ontrack = (event) => {
//   const stream = event.streams[0];
//   console.log("ğŸ“¡ Remote stream received:", stream);
//   console.log("ğŸ§ Audio tracks:", stream.getAudioTracks());
//   if (remoteVideoRef.current) {
//     remoteVideoRef.current.srcObject = stream;
//   }
// };
pc.ontrack = (event) => {
  const stream = event.streams[0];

  // ë¹„ë””ì˜¤ ì—°ê²°
  if (remoteVideoRef.current) {
    remoteVideoRef.current.srcObject = stream;
  }

  // ì˜¤ë””ì˜¤ ì—°ê²°
  if (remoteAudioRef.current) {
    remoteAudioRef.current.srcObject = stream;
  }

  console.log("ğŸ“¡ Remote stream received:", stream);
  console.log("ğŸ§ Audio tracks:", stream.getAudioTracks());
};


    return pc;
  };

  // ë¡œì»¬ ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¼ ê°€ì ¸ì˜¤ê¸°
  const startLocalStream = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true, video: isVideoOn });
      localStreamRef.current = stream;

      if (localVideoRef.current) {
        localVideoRef.current.srcObject = stream;
      }

      // ìŒì„± ë³¼ë¥¨ ë¡œê·¸ìš© AudioContext (í•„ìš”ì‹œ ì‚¬ìš©)
      /*
      const audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      source.connect(analyser);
      analyser.fftSize = 256;
      const dataArray = new Uint8Array(analyser.frequencyBinCount);

      const logVolume = () => {
        analyser.getByteFrequencyData(dataArray);
        const avg = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;
        // console.log("ğŸ™ï¸ í˜„ì¬ ìŒì„± ë ˆë²¨:", avg.toFixed(2));
        requestAnimationFrame(logVolume);
      };
      logVolume();
      */

      toast({ title: "Media Ready", description: "Camera and microphone access granted." });
      return stream;
    } catch {
      toast({ title: "Media Error", description: "ì¹´ë©”ë¼ ë° ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.", variant: "destructive" });
      throw new Error("Media permission denied");
    }
  };

  // í˜¸ì¶œ ì‹œì‘
  const call = async () => {
    if (!ws.current || !targetId) return;

    setCallStatus('connecting');

    const stream = await startLocalStream();

    peerConnectionRef.current = createPeerConnection();

    stream.getTracks().forEach(track => {
      peerConnectionRef.current?.addTrack(track, stream);
    });

    const offer = await peerConnectionRef.current.createOffer();
    await peerConnectionRef.current.setLocalDescription(offer);

    ws.current.send(JSON.stringify({
      type: 'offer',
      to: targetId,
      sdp: offer,
    }));

    setCallStatus('calling');
  };

  // offer ì²˜ë¦¬ ë° answer ìƒì„±
  const handleOffer = async (message: any) => {
    setCallStatus('connecting');

    const stream = await startLocalStream();

    peerConnectionRef.current = createPeerConnection();

    stream.getTracks().forEach(track => {
      peerConnectionRef.current?.addTrack(track, stream);
    });

    await peerConnectionRef.current.setRemoteDescription(new RTCSessionDescription(message.sdp));

    const answer = await peerConnectionRef.current.createAnswer();
    await peerConnectionRef.current.setLocalDescription(answer);

    ws.current?.send(JSON.stringify({
      type: 'answer',
      to: message.from || userId,
      sdp: answer,
    }));

    setCallStatus('connected');
  };


  // í†µí™” ì¢…ë£Œ
  const endCall = () => {
    peerConnectionRef.current?.close();
    peerConnectionRef.current = null;

    localStreamRef.current?.getTracks().forEach(t => t.stop());
    localStreamRef.current = null;

    if (localVideoRef.current) localVideoRef.current.srcObject = null;
    if (remoteVideoRef.current) remoteVideoRef.current.srcObject = null;

    setCallStatus('idle');
    toast({ title: "Call Ended", description: "í†µí™”ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤." });
  };

  // ë§ˆì´í¬ ìŒì†Œê±° í† ê¸€
  const toggleMute = () => {
    if (!localStreamRef.current) return;
    const audioTrack = localStreamRef.current.getAudioTracks()[0];
    if (!audioTrack) return;

    audioTrack.enabled = !audioTrack.enabled;
    setIsMuted(!audioTrack.enabled);
  };

  // ë¹„ë””ì˜¤ í† ê¸€
  const toggleVideo = () => {
    if (!localStreamRef.current) return;
    const videoTrack = localStreamRef.current.getVideoTracks()[0];
    if (!videoTrack) return;

    videoTrack.enabled = !videoTrack.enabled;
    setIsVideoOn(videoTrack.enabled);
  };

  // ìƒíƒœë³„ ë°°ì§€ ìƒ‰ìƒ
  const getStatusColor = () => {
    switch (callStatus) {
      case 'connected': return 'bg-green-600 text-white';
      case 'connecting': return 'bg-yellow-500 text-black';
      case 'calling': return 'bg-yellow-400 text-black';
      default: return 'bg-gray-300 text-black';
    }
  };

  return (
    <div className="min-h-screen p-6 bg-gray-50">
      <div className="max-w-5xl mx-auto space-y-6">
<audio ref={remoteAudioRef} autoPlay />

        <Card>
          <CardHeader className="flex justify-between items-center">
            <div className="flex items-center gap-2">
              <Phone className="w-6 h-6 text-green-600" />
              <CardTitle>Internal Call System</CardTitle>
            </div>
            <Badge className={getStatusColor()}>
              {callStatus === 'idle' ? 'Ready' : callStatus.charAt(0).toUpperCase() + callStatus.slice(1)}
            </Badge>
          </CardHeader>
        </Card>

        <Card>
          <CardHeader>
            <CardTitle>Call Setup</CardTitle>
          </CardHeader>
          <CardContent className="flex flex-col gap-3">
            <Input
              placeholder="Your ID"
              value={userId}
              onChange={(e) => setUserId(e.target.value.trim())}
              disabled={isConnected}
            />
            <Input
              placeholder="Target ID"
              value={targetId}
              onChange={(e) => setTargetId(e.target.value.trim())}
              disabled={callStatus !== 'idle'}
            />

            <div className="flex gap-4">
              <Button
                onClick={call}
                disabled={!userId || !targetId || callStatus !== 'idle' || !isConnected}
              >
                Call
              </Button>
              <Button
                variant="destructive"
                onClick={endCall}
                disabled={callStatus === 'idle'}
              >
                Hang Up
              </Button>
            </div>
          </CardContent>
        </Card>

        <Separator />

        <Card>
          <CardHeader>
            <CardTitle>Transcription</CardTitle>
          </CardHeader>
          <CardContent>
            <div className="min-h-[100px] p-2 border rounded bg-white text-black whitespace-pre-wrap">
              {transcript || "ìŒì„±ì„ ì¸ì‹ ì¤‘ì…ë‹ˆë‹¤..."}
            </div>
          </CardContent>
        </Card>

        <Card>
  <CardHeader>
    <CardTitle>My Speech</CardTitle>
  </CardHeader>
  <CardContent>
    <div className="min-h-[100px] p-2 border rounded bg-white text-black whitespace-pre-wrap">
      {transcript || "ë‚´ ìŒì„±ì„ ì¸ì‹ ì¤‘..."}
    </div>
  </CardContent>
</Card>

<Card>
  <CardHeader>
    <CardTitle>Partner's Speech</CardTitle>
  </CardHeader>
  <CardContent>
    <div className="min-h-[100px] p-2 border rounded bg-white text-black whitespace-pre-wrap">
      {receivedTranscript || "ìƒëŒ€ë°© ìŒì„±ì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘..."}
    </div>
  </CardContent>
</Card>


        <div className="grid grid-cols-2 gap-4">
          <Card>
            <CardHeader><CardTitle>Local Video</CardTitle></CardHeader>
            <CardContent>
              <video ref={localVideoRef} autoPlay muted playsInline className="w-full rounded" />
            </CardContent>
          </Card>

          <Card>
            <CardHeader><CardTitle>Remote Video</CardTitle></CardHeader>
            <CardContent>
              <video ref={remoteVideoRef} autoPlay playsInline className="w-full rounded" />
            </CardContent>
          </Card>
        </div>

        <div className="flex gap-4 justify-center">
          <Button onClick={toggleMute} disabled={callStatus === 'idle'}>
            {isMuted ? <MicOff className="w-5 h-5" /> : <Mic className="w-5 h-5" />}
          </Button>
          <Button onClick={toggleVideo} disabled={callStatus === 'idle'}>
            {isVideoOn ? <Video className="w-5 h-5" /> : <VideoOff className="w-5 h-5" />}
          </Button>
        </div>

      </div>
    </div>
  );
}
